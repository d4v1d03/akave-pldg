# Quick Start Guide

## ğŸš€ Get Started in 5 Minutes

### Step 1: Set Up Environment

```bash
cd /Users/amitpandey/pldg/akave/project

# Copy environment template
cp .env.example .env

# Edit .env and add your Akave private key
nano .env  # or use your preferred editor
```

Required: Add your `AKAVE_PRIVATE_KEY` in `.env`

### Step 2: Start Services

```bash
# Start all services (API, Worker, PostgreSQL, Redis)
docker-compose up -d

# Check status
docker-compose ps

# View logs
docker-compose logs -f
```

### Step 3: Test the API

```bash
# Test health
curl http://localhost:8000/health

# Create a bucket
curl -X POST http://localhost:8000/buckets \
  -H "Content-Type: application/json" \
  -d '{"bucket_name": "my-test-bucket"}'

# Save the job_id from response, then check status:
curl http://localhost:8000/buckets/jobs/{job_id}

# List all buckets
curl http://localhost:8000/buckets
```

### Step 4: Run Automated Tests

```bash
# Install httpx for testing
pip install httpx requests

# Run test suite
python test_api.py
```

## ğŸ“Š What Happens Behind the Scenes?

1. **API receives request** â†’ Creates job in PostgreSQL â†’ Returns job_id instantly
2. **Redis queue** â†’ Holds the job
3. **Celery worker** â†’ Picks up job â†’ Calls Akave SDK â†’ Creates bucket on blockchain
4. **Database updated** â†’ Status changes: queued â†’ processing â†’ completed
5. **You poll** â†’ GET /buckets/jobs/{job_id} to see progress

## ğŸ” Monitoring

### Check Worker Status
```bash
# Worker logs
docker-compose logs -f worker

# API logs
docker-compose logs -f api
```

### Database Queries
```bash
# Connect to PostgreSQL
docker-compose exec postgres psql -U akave -d akave_platform

# Check jobs
SELECT bucket_name, status, created_at FROM bucket_jobs;

# Count by status
SELECT status, COUNT(*) FROM bucket_jobs GROUP BY status;
```

### Redis Inspection
```bash
# Connect to Redis
docker-compose exec redis redis-cli

# Check queue length
LLEN celery

# View keys
KEYS *
```

## ğŸ› ï¸ Troubleshooting

### API not starting?
```bash
docker-compose logs api
# Check for port conflicts on 8000
```

### Worker not processing?
```bash
# Restart worker
docker-compose restart worker

# Check environment variables
docker-compose exec worker env | grep AKAVE
```

### Database issues?
```bash
# Reinitialize database
docker-compose down -v
docker-compose up -d
```

## ğŸ“ˆ Scaling

```bash
# Run 3 workers in parallel
docker-compose up -d --scale worker=3

# Check all workers
docker-compose ps worker
```

## ğŸ§¹ Cleanup

```bash
# Stop services
docker-compose down

# Remove volumes (deletes database data)
docker-compose down -v
```

## ğŸ“ Project Structure

```
project/
â”œâ”€â”€ api.py              # FastAPI application
â”œâ”€â”€ celery_app.py       # Celery configuration
â”œâ”€â”€ worker.py           # Worker tasks
â”œâ”€â”€ schema.sql          # Database schema
â”œâ”€â”€ requirements.txt    # Python dependencies
â”œâ”€â”€ docker-compose.yml  # Service orchestration
â”œâ”€â”€ Dockerfile          # Container image
â”œâ”€â”€ test_api.py         # Test suite
â””â”€â”€ README.md           # Full documentation
```

## ğŸ¯ Next Steps

1. âœ… Create buckets via API
2. ğŸ“Š Add monitoring dashboard
3. ğŸ“ Implement file upload
4. ğŸ§Š Integrate Apache Iceberg
5. â˜¸ï¸ Deploy to Kubernetes

---

